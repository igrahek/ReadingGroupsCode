{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks for Structured Data\n",
    "\n",
    "** *Based on Murphy 2022 (Chapter 13) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction (13.1)\n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Logistic and linear regression assume that the input-output mapping is linear\n",
    "- We can make these models more flexible by performing feature transformation (aka basis function expansion):\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "f(x;\\theta) = W\\phi(x) + b\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "- The model is still linear in the parameters $\\theta = (W,b)$, so model fitting is easy since the negative log likelihood is convex. However, we still have to specify the feature transofrmation $\\phi(x)$ by hand. \n",
    "\n",
    "- Instead, we can endow the frature extractor with its own parameters:\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "f(x;\\theta) = W\\phi(x;\\theta) + b\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "Now $\\theta = (\\theta_1, \\theta_2)$, where $\\theta_1$ are the parameters of the linear model ($\\theta_1 = (W,b)$), and $\\theta_2$ are the parameters of the feature extractor. \n",
    "\n",
    "We can repeat this recursively to create more complex functions:\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "f(x;\\theta) = f_{L}(f_{L-1}(...(f_{}(x))...))\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "Where $f_l = f(x,\\theta_l)$\n",
    "\n",
    "**This is the key idea behind deep neural networsks (DNNs)**\n",
    "\n",
    "- DNN exmplass a larger family of model sin which we compose differentiable functions into any kind of directed acyclic graph (DAG) mapping input to output. \n",
    "\n",
    "- $f(x;\\theta) = f_{L}(f_{L-1}(...(f_{}(x))...))$ is the simplest example where DAG is a chain, and this is known as a **feedforward neural network** of **multilayer perceptron (MLP)**\n",
    "\n",
    "- An MLP assumes that the input is a fixed-dimensional vector, and it is common to call such data **structured data** or **tabular data**. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer perceptrons (13.2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Perceptron is a deterministic version of logistic regression. It a mapping of the following form:\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "f(x:\\theta) = \\mathbb{I}(w^Tx + b \\ge0) = H(w^Tx + b)\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "- $H(A)$ is a heavside step function, also known as a linear threshold function. Since decision boundaries represented by perceptrons are linear, they are very limited in what they can represent. This was noted by Minsy & Papert (1969). \n",
    "\n",
    "- To solve the XOR problem we need a **multilayer perceptron (MLP)**. We add hidden units and thus have multiple layers. Then MLPs can solve problems which are not linearly separable. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differentiable MLPs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The Havside function is not differentiable, so we can't train the model. \n",
    "\n",
    "- Thus we use a differentiable **activation function** $\\phi: \\mathbb{R} \\rightarrow \\mathbb{R}$. Hidden units $z_{l}$ at each layer are linear transformation of the hidden units at the previous layer passed through the activation function:\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "z_{l} = \\phi-{l}(b_{l} + W_{l}z_{l-1})\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "or in scalar form:\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "z_{k,l} = \\phi_{l}(b_{kl} + \\sum_{j=1}^{K_{l-1}}W_{jkl}z_{jl-1})\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "- The quantity passed to the activation function is called the **pre-activations**:\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "a_{l} = b_{l} + W_{l}z_{l-1}\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "- so:\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "z_{l} = \\phi_{l}(a_{l})\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "- If we componse L of these fuctnions together we can compute the gradient of the output wrt the parameters in each layer using the cahing rule, also non as **backpropagation**.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If we use a linear activation function $\\phi_{l}(a) = c_{l}a$, then the model reduces to a regular linear model.\n",
    "\n",
    "- In the early days of neural networks the common nonlinear activation function was the sigmoid which is a smooth approximation to the Heaviside function:\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "\\phi(a) = \\sigma(a) = \\frac{1}{1+e^{-a}}\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "- The sigmoid saturates at 0 & 1. We can also use the tanh (-1 1). For these functions, in the saturated regimes, the gradient of the output wrt the input will be close to zeo, so any gradient signal from higher layers will not be able to propagate back to earlier layes. This is the **vanishing gradient problem** making it hard to rain the model using gradient descent. \n",
    "\n",
    "- The most common solution is the use of teh **rectified linear unit (ReLU)**:\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "ReLU(a) = \\max(a,0) = a\\mathbb{I}(a \\ge 0)\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "- The ReLU function turns off negative inputs and passes positive inputs unchanged. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play around with the activation functions here: https://playground.tensorflow.org/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- MLP with one hidden layer is a **universal function approximator**. It can model any suitably smooth function given enough hidden unites to an arbitrary degree of accuracy. Each hidden unit can specify half plane, so a sufficiantly large comibnation of these can carve up any region of space. \n",
    "\n",
    "- However, deep netwroks work better than shallow ones. THe later layers can levarage the features that are learned by earlier layers, and the function is defined in a compositional or hierarchical manner."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation (13.3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Backpropagation algorithm - used to compute the gradient of a loss functoin applied to the output of the network wrt the parameters in each layer\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward vs. reverse mode differentiation (13.3.1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We first consider a linear chain of stacked layers (as in an MLP). \n",
    "- In this case backprop is equivalent to repeated application of the chain rule of calculus:\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "\\frac{d}{dx}f(u(x)) = \\frac{du}{dx}\\frac{df(u)}{du}\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "\n",
    "- Consider the mapping:\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "o = f(x)\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "x \\in \\mathbb{R}^n\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "o \\in \\mathbb{R}^m\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "- Where:\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "f = f_4 \\circ f_3 \\circ f_2 \\circ f_1\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "\n",
    "- We can compute the gradient of the output wrt the input using the chain rule:\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial o}{\\partial x} = \\frac{\\partial o}{\\partial x_4} \\frac{\\partial x_4}{\\partial x_3} \\frac{\\partial x_3}{\\partial x_2} \\frac{\\partial x_2}{\\partial x} = \\frac{\\partial f_4(x_4)}{\\partial x_4} \\frac{\\partial f_3(x_3)}{\\partial x_3} \\frac{\\partial f_2(x_2)}{\\partial x_2} \\frac{\\partial f_1(x)}{\\partial x} = J_{f_4}(x_4)J_{f_3}(x_3)J_{f_2}(x_2)J_{f_1}(x)\n",
    "\\end{aligned}\n",
    "$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Jacobian $J_f(x)$ can be computed efficiently. \n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "J_f(x) = \\frac{\\partial f(x)}{\\partial x} = \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial f_1(x)}{\\partial x_1} & \\frac{\\partial f_1(x)}{\\partial x_2} & \\dots & \\frac{\\partial f_1(x)}{\\partial x_n} \\\\\n",
    "\\frac{\\partial f_2(x)}{\\partial x_1} & \\frac{\\partial f_2(x)}{\\partial x_2} & \\dots & \\frac{\\partial f_2(x)}{\\partial x_n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial f_m(x)}{\\partial x_1} & \\frac{\\partial f_m(x)}{\\partial x_2} & \\dots & \\frac{\\partial f_m(x)}{\\partial x_n} \\\\\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "\\frac{\\partial f_1(x)}{\\partial x} \\\\\n",
    "\\frac{\\partial f_2(x)}{\\partial x} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial f_m(x)}{\\partial x} \\\\\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "\\nabla f_1(x)^T \\\\\n",
    "\\nabla f_2(x)^T \\\\\n",
    "\\vdots \\\\\n",
    "\\nabla f_m(x)^T \\\\\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "\\frac{\\partial f(x)}{\\partial x_1} & \\frac{\\partial f(x)}{\\partial x_2} & \\dots & \\frac{\\partial f(x)}{\\partial x_n} \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "- Note that the gradient $\\nabla f(x)$ is a column vector, awhile the Jacobian $J_f(x)$ is a row vector. Thus we technically have:\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "\\nabla f(x)^T = J_f(x)\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "**Vector-Jacobian product (VJP)**\n",
    "- We can extract the i'th rom from $J_f(x)$ by using a vector Jacobian product of the form: $e_i^TJ_f(x)$ where $e_i$ is the i'th unit basis vector ($e_i \\in \\mathbb{R}^m$).\n",
    "\n",
    "**Jacobian-vector product (JVP)**\n",
    "- We can extract the j'th column from $J_f(x)$ by using a Jacobian vector product of the form: $J_f(x)e_j$ where $e_j \\in \\mathbb{R}^n$.\n",
    "\n",
    "- Thus, the computation of $J_f(x)$ reduces to either $n$ JVPs or $m$ VJPs.\n",
    "\n",
    "**When $n < m$**\n",
    "\n",
    "- It is more efficient to compute the JVPs than the VJPs.\n",
    "- This is **forward mode differentiation**.\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "J_f(x)v = J_{f_4}(x_4)J_{f_3}(x_3)J_{f_2}(x_2)J_{f_1}(x_1)v \n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "**When $n > m$**\n",
    "\n",
    "- It is more efficient to compute the VJPs than the JVPs.\n",
    "- This is **reverse mode differentiation**.\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "u^TJ_f(x) = u^TJ_{f_4}(x_4)J_{f_3}(x_3)J_{f_2}(x_2)J_{f_1}(x_1)\n",
    "\\end{aligned}\n",
    "$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
